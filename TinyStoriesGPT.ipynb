{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.data.utils as tt_utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_train = 30000\n",
    "limit_val = 100\n",
    "\n",
    "data = dataset['train'][:limit_train]['text']\n",
    "data_val = dataset['validation'][:limit_val]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tokenizer with text normalization\n",
    "tokenizer = tt_utils.get_tokenizer('basic_english')\n",
    "\n",
    "# Normalize the stories\n",
    "normalized_stories = [tokenizer(story) for story in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "stories_dictionary = Counter()\n",
    "\n",
    "for story in normalized_stories:\n",
    "  stories_dictionary.update(set(story))\n",
    "\n",
    "stories_counts = sorted([(x, stories_dictionary[x]) for x in stories_dictionary], key=lambda x: -x[-1])\n",
    "\n",
    "word2index = {pair[0]:i for i, pair in enumerate(stories_counts, 1)}; word2index['<pad>'] = 0\n",
    "index2word = {i:word[0] for i, word in enumerate(stories_counts, 1)}; index2word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2index(text_list):\n",
    "  output = [word2index[word] for word in text_list]\n",
    "  return output\n",
    "\n",
    "def index2list(index_list):\n",
    "  output = [index2word[index] for index in index_list]\n",
    "  return output\n",
    "\n",
    "def input_target_pair(input, maxlen=256):\n",
    "  output = []\n",
    "  for i in range(0, len(input)-(maxlen+1)):\n",
    "    output.append((torch.tensor(input[i:i+maxlen]).to(device),\n",
    "                   torch.tensor(input[i+maxlen]).to(device)))\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process list of words to a dataloader of tuples (input, target)\n",
    "def get_dataloader(stories, batch_size=16, maxlen=256):\n",
    "  # Tokenize and pad lists of stories\n",
    "  token_stories = [[0] * (maxlen-1) + list2index(story) for story in stories]\n",
    "\n",
    "  # Convert tokenized stories to (input, target) tuples\n",
    "  input_target_pairs = [input_target_pair(t_story) for t_story in token_stories]\n",
    "\n",
    "  # Store all examples in one list\n",
    "  inpurt_target_heap = []\n",
    "  for pair in input_target_pairs:\n",
    "    inpurt_target_heap += pair\n",
    "\n",
    "  # Generate dataloader\n",
    "  dataloader = DataLoader(inpurt_target_heap, batch_size=batch_size, shuffle=True)\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "  mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "  mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "  return mask\n",
    "\n",
    "\n",
    "def create_mask(src):\n",
    "  PAD_IDX = 0\n",
    "\n",
    "  src_seq_len = src.shape[0]\n",
    "\n",
    "  src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool)\n",
    "\n",
    "  src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "  return src_mask, src_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, vocab_size, dropout):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.dropout(self.embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, maxlen):\n",
    "    super().__init__()\n",
    "\n",
    "    den = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model)).to(device)\n",
    "    pos = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1).to(device)\n",
    "    self.encoding = torch.zeros(maxlen, d_model).to(device)\n",
    "    self.encoding[:, 0::2] = torch.sin(pos * den)\n",
    "    self.encoding[:, 1::2] = torch.cos(pos * den)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, dropout):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = d_model // num_heads\n",
    "\n",
    "    # Define separate linear transformations for query, key, and value for each head\n",
    "    self.query = nn.Linear(d_model, d_model)\n",
    "    self.key = nn.Linear(d_model, d_model)\n",
    "    self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # Output linear layer for each head\n",
    "    self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, seq_len, emb_dim = x.size()\n",
    "\n",
    "    # Linear transformations for query, key, and value for each head\n",
    "    Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2); Q = self.dropout(Q)\n",
    "    K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2); K = self.dropout(K)\n",
    "    V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2); V = self.dropout(V)\n",
    "\n",
    "    # Calculate attention scores and attention weights for each head with scaling factor\n",
    "    scores = torch.matmul(Q, K.transpose(-1, -2)) / (self.head_dim ** 0.5)\n",
    "\n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "    attended_values = torch.matmul(attention_weights, V)\n",
    "\n",
    "    # Reshape and concatenate attended values from all heads\n",
    "    attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "    # Apply output linear layer and residual connection\n",
    "    output = self.out(attended_values)\n",
    "    output += x\n",
    "\n",
    "    # Apply layer normalization\n",
    "    output = self.norm(output)\n",
    "    return output, attention_weights.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title `FeedForward` class\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, d_model, d_ff, dropout):\n",
    "    super().__init__()\n",
    "\n",
    "    # Linear layers\n",
    "    self.pickles = nn.Linear(d_model, d_ff)\n",
    "    self.tomatoes = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Weights normalization\n",
    "    nn.init.kaiming_normal_(self.pickles.weight, nonlinearity='relu')\n",
    "    nn.init.kaiming_normal_(self.tomatoes.weight, nonlinearity='relu')\n",
    "\n",
    "  def forward(self, x):\n",
    "    pickle = self.pickles(x)\n",
    "    pickle = F.relu(pickle)\n",
    "\n",
    "    tomato = self.tomatoes(pickle)\n",
    "    tomato = self.dropout(tomato)\n",
    "\n",
    "    output = self.norm(tomato)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHDecoderTransformer(nn.Module):\n",
    "  def __init__(self, d_model, maxlen, vocab_size, dropout, n_heads, d_ff, n_att):\n",
    "    super().__init__()\n",
    "\n",
    "    # Classes\n",
    "    self.embedding = TokenEmbedding(d_model, vocab_size, dropout).to(device)\n",
    "    self.posencoding = PositionalEncoding(d_model, maxlen).to(device)\n",
    "    self.sequential_attention = [MultiHeadSelfAttention(d_model, n_heads, dropout).to(device) for _ in range(n_att)]\n",
    "    self.neuralnet = FeedForward(d_model, d_ff, dropout).to(device)\n",
    "\n",
    "    self.flatten = lambda x: x.view(x.size(0), -1)\n",
    "    self.out = nn.Linear(maxlen * d_model, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    embeded = self.embedding(x)\n",
    "    posencoded = self.posencoding(embeded)\n",
    "    att_Ws = []\n",
    "\n",
    "    attended = posencoded\n",
    "    for lil_attention in self.sequential_attention:\n",
    "      attended, att_W = lil_attention(attended)\n",
    "      att_Ws.append(att_W)\n",
    "\n",
    "    boring = self.neuralnet(attended)\n",
    "    flat = self.flatten(boring)\n",
    "    output = self.out(flat)\n",
    "    return output, att_Ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initializatopn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "D_MODEL = 256\n",
    "VOCAB_SIZE = len(word2index)\n",
    "MAXLEN = 32\n",
    "NUM_HEADS = 8\n",
    "D_HIDDEN = 512\n",
    "N_ATT = 8\n",
    "DROPOUT = .1\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimus = MHDecoderTransformer(D_MODEL, MAXLEN, VOCAB_SIZE, DROPOUT, NUM_HEADS, D_HIDDEN, N_ATT).to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_params = count_parameters(Optimus)\n",
    "print(f\"Number of trainable parameters in the model: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = get_dataloader(normalized_stories[:24000], BATCH_SIZE, MAXLEN)\n",
    "testloader = get_dataloader(normalized_stories[24000:], 32, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=Optimus.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, plot_loss, print_loss):\n",
    "  model.train()\n",
    "\n",
    "  LOSS = 0\n",
    "  plot = []\n",
    "\n",
    "  for i, (input, target) in enumerate(loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits, _ = model(input)\n",
    "\n",
    "    loss = criterion(logits, target)\n",
    "    LOSS += loss.item()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i+1)%print_loss == 0:\n",
    "      print(f'Training epoch {i+1}/{len(loader)}: {(LOSS/i):.5f}')\n",
    "\n",
    "    if i%plot_loss == 0:\n",
    "      plot.append(loss.item())\n",
    "\n",
    "  LOSS /= len(loader)\n",
    "  return LOSS, plot\n",
    "\n",
    "def get_time(epoch_time):\n",
    "  minutes = int(epoch_time) // 60\n",
    "  seconds = epoch_time - minutes*60\n",
    "  return f'Time taken: {minutes} m. {seconds:.1f} s.'\n",
    "\n",
    "def eval_model(model, loader, limit=1):\n",
    "  model.eval()\n",
    "\n",
    "  LOSS = 0\n",
    "  loss_list = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, (input, target) in enumerate(loader, 1):\n",
    "      logits, _ = Optimus(input)\n",
    "\n",
    "      loss = criterion(logits, target)\n",
    "\n",
    "      LOSS += loss\n",
    "      loss_list.append(loss)\n",
    "\n",
    "      if (i / len(loader)) > limit:\n",
    "        break\n",
    "\n",
    "  LOSS /= len(loader) * limit\n",
    "  return LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "epochs = 1\n",
    "print_loss = 100\n",
    "plot_loss = 100\n",
    "loss_list = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "  start_time = time.time()\n",
    "  loss, plot = train_epoch(Optimus, trainloader, plot_loss, print_loss)\n",
    "  loss_list += plot\n",
    "  epoch_time = time.time() - start_time\n",
    "  print(f'Epoch #{epoch}: Loss = {loss:.5f}\\n{get_time(epoch_time)}')\n",
    "  validation_loss = eval_model(Optimus, testloader)\n",
    "  print(f'Validation loss = {validation_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model's weights\n",
    "PATH = '/content/drive/MyDrive/abacaba.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_temperature(logits, temperature=1.0):\n",
    "    return logits / temperature\n",
    "\n",
    "input, target = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token = 250\n",
    "temperature = 1.\n",
    "\n",
    "current = input[0].unsqueeze(0)\n",
    "stack = input[0].tolist()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(max_token):\n",
    "        ex, _ = Optimus(current)\n",
    "\n",
    "        # Apply temperature to the logits before sampling\n",
    "        scaled_logits = apply_temperature(ex[0], temperature)\n",
    "        probabilities = torch.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "        # Sample the next token using the probabilities distribution\n",
    "        ex = torch.multinomial(probabilities, num_samples=1).squeeze()\n",
    "\n",
    "        stack.append(ex.item())\n",
    "        current = torch.tensor([stack[-32:]]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = index2list(stack)\n",
    "\n",
    "# Create a single string with elements separated by a space\n",
    "sentence_string = ' '.join(sentence)\n",
    "\n",
    "# Set the number of elements to include in each line before inserting a newline character\n",
    "elements_per_line = 32\n",
    "\n",
    "# Split the sentence into chunks of 'elements_per_line' elements\n",
    "chunks = [sentence[i:i+elements_per_line] for i in range(0, len(sentence), elements_per_line)]\n",
    "\n",
    "# Join the chunks with a newline character to create line breaks\n",
    "formatted_sentence = '\\n'.join(' '.join(chunk) for chunk in chunks)\n",
    "\n",
    "print(formatted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw generated text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and they watched as the rain poured down around them . after the rain stopped , timmy and lily continued their adventure . they found a butterfly and a caterpillar , and\n",
    "they found heading on that lily ' s fun . they are happy and they are some toys . billy , mom , dad a kind and want some food . snack\n",
    "is so much they have fun !"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
